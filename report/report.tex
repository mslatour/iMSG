\documentclass[a4paper]{article}
\usepackage[latin1]{inputenc}
\usepackage[english]{babel}
\usepackage{graphicx}
%\usepackage{tikz-qtree}

\usepackage[margin=2cm]{geometry}

\title{iMSG: Guiding language evolution by the need to express}
\author{Michael Cabot\\6047262 \and Sander Latour\\5743044}

\begin{document}
\maketitle
\section{Introduction}
\cite{zuidema2003poverty} \cite{batali1999negotiation}
In language learning, communication between a parent and a child typically starts with a semantic message (from now on refered to as the \emph{intention}) that the parent wants to share with the child. In order to communicate the intention, the parent needs to express it into a specific language formalism according to grammar rules known to the parent. The child then receives that expression in combination with the expressed intention. Although many communication happens between a parent and a child where there is not an explicit transfer of the intention, we focus on the part of language learning where the verbalized intention is somehow pointed out in other ways than verbal language (e.g. by ensuring a similar observation). Based on earlier communications together with the current one, the child attempts to induce structural grammar rules on how to parse and generate expressions.
\textbf{TODO: language evolution, iterative process between parent and child. progress made, especially by zuidema in reducing number of restrictions and assumptions. however problem in ever-decreasing grammar size}

The authors believe that the necessity to express a certain set of intentions will take care of the decreasing complexity of the grammar in a more natural way. The hypothesis is that by providing the parent with a list of intentions it needs to express, the iterated language evolution will optimize the language without reducing it to triviality. In order to test this hypothesis, this paper presents a semantic-driven iterated language learning framework based on both the work of \cite{zuidema2003poverty} on language evolution and the work of \cite{batali1999negotiation} on inducing semantic grammars from observations.

This paper is structured as follows. Section \ref{sec:system_overview} describes the system that was build. In section \ref{sec:experiments}, the experiments are described that were conducted to test the hypothesis. The results of these experiments are discussed in section \ref{sec:results}. Section \ref{sec:conclusion} concludes the results in this paper. And in section \ref{sec:discussion} the research is discussed.

\section{Related work}
%explain batali
%explain zuidema
%explain how this paper differs
\section{Notation and Vocabulary}
\begin{description}
    \item[formula] A formula $f$ consists of a predicate, denoted as $pred(f)$, and zero or more arguments, denoted as $args(f)$. The number of arguments is defined by the type of the formula, denoted by $type(f)$, which in this paper is either \verb|PropertyFormula| or \verb|RelationFormula| with one and two arguments respectively. For example, the formula \verb|(snake, 1)| has $pred($\verb|(snake,1)|$)$ = \verb|snake|, $args($\verb|(snake,1)|$)$ = $[1]$ and $type($\verb|(snake, 1)|$)$ = \verb|ProperyFormula|.
    \item[intention] An intention is a set of formulas that as a whole contains some semantic message. For example, an intention could contain \verb|[(tweety, 1), (see, 1, 2), (pussycat, 2)]| to express that tweety sees a pussycat. Intentions can be combined with other intentions by merging the sets.
    \item[rule] A rule $r$ is similar to an unnormalized PCFG rule. The left-hand-side of the rule, denoted by $lhs(r)$, is an intention. The right-hand-side of the rule, denoted as $rhs(r)$, is a set of intentions that together combine into $lhs(r)$. Linked to the rule is the cost of using the rule, denoted by $cost(r)$, which inversely relates to the probability of that rule being used.
    \item[lexical rule] A lexical PCFG rule $r_l$ differs from normal rules in that $lhs(r_l)$ contains only one formula and $rhs(r_l)$ is a string of characters (i.e. a word). In this paper, $type(lhs(r_l))$ will denote the type of the single formula in $lhs(r_l)$.
    \item[grammar] A grammar $G$ consists of all rules and lexical rules. The subgrammar $G_l$ is a subset of $G$ and contains only the lexical rules from $G$.
% argument mapping
\end{description}
\section{System overview}
\label{sec:system_overview}
In order to explore the effect of this paper's approach and the validaty of the hypothesis, a computer simulation was designed and implemented. The computer simulation consisted of three main parts: \emph{generating intention}, \emph{expressing intention} and \emph{inducing structure}. These parts will be described in the following sections.
% image depicting the general outline (parent -> child, iterated loop)
\subsection{Generating intention}
Three different methods were developed to obtain an intention to express. The first one is simply by taking a uniform random sample from a fixed set of intentions. The other two methos will now be described.
\subsubsection{Meaning sampling with templates}
A variation to having a predefined set of intentions is to only sample from predefined templates. These templates consist of placeholders that specify the type of formulas that can be ingested and the arguments that the formula wil have. For example, the template \verb|[(PropertyFormula, 1), (RelationFormula, 1, 2), (PropertyFormula, 2)]| allows for intentions that contain two properties and a relationship between them. A possible instantiation of this template is \verb|[(snake, 1), (bite, 1, 2), (pig, 2)]|. The templates ensure a certain complexity in the intention, without fully specifying it. The instantiation of the template is done by sampling formulas that fit the specified type according to the grammar, which is described in the following algorithm:
\begin{enumerate}
    \item For each placeholder in template:
        \begin{enumerate}
            \item For rule in $G_l$
        \end{enumerate}
\end{enumerate}

\subsubsection{Template sampling}

\subsection{Parent: Verbalize meanings}
\subsubsection{Find the words to say}
\subsubsection{Verbalization cost}
\subsection{Child: Learning grammar} % michael
\subsubsection{ViterbiX} % michael
\section{Experiments}
\label{sec:experiments}
\section{Results}
\label{sec:results}
\section{Conclusion}
\label{sec:conclusion}
\section{Discussion}
\label{sec:discussion}

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}
